{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Ctrl+Shift+i + эта фунция - чтоб KAGGLE не засыпал\n# function ClickConnect(){\n# console.log(\"Working\");\n# document.querySelector(\"#site-content > div.AppView-sc-16eb2j.kDpcJw > div.App_Body-sc-16c8j4p.hxOBfv > div.Layout_Body-sc-6piylv.bXAYPy > div > div > div > div.ToolbarContainer_Body-sc-2h8iu7.fhvgBU > div.ToolbarActionsRight_Body-sc-1muxuky.eJnDPK > button:nth-child(3) > i\").click()\n# }setInterval(ClickConnect,60000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/jqeric/detectors-new-sota-based-mmdetection  \nhttps://zhuanlan.zhihu.com/p/119456263  \nhttps://www.kaggle.com/vadimtimakin/mmdetection-train - пример обучения скриптом"},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvcc --version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Установка MMdetection \nпо мотивам https://mmdetection.readthedocs.io/en/latest/get_started.html#installation  \nи https://www.kaggle.com/vadimtimakin/mmdetection-train"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex\n#clear_output(wait=True)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install mmcv-full==latest+torch1.6.0+cu101 -f https://download.openmmlab.com/mmcv/dist/index.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install -r requirements/build.txt\n!pip install -v -e .  # or \"python setup.py develop\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# install Pillow 7.0.0 back in order to avoid bug in colab\n!pip install Pillow==7.0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install \"git+https://github.com/open-mmlab/cocoapi.git#subdirectory=pycocotools\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RESTART KERNEL!!!!\nfrom IPython.core.display import HTML\nHTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check MMDetection installation\nimport mmdet\nprint(mmdet.__version__)\n\n# Check mmcv installation\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('----- MMDetection installation DONE -----')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"******************"},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 20726","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport torch\nimport random\nimport sys\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import clear_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://github.com/NVIDIA/apex"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed=20726):\n    from mmdet.apis import set_random_seed\n    \"\"\"Sets the random seeds.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    set_random_seed(seed)\n\nset_seed(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/testphotos'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# faster_rcnn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# download the checkpoint from model zoo and put it in `checkpoints/`\n# url: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n!wget http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth #-P /home/alex/download","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mmdet.apis import init_detector, inference_detector\n\nconfig_file = '/kaggle/working/mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n# download the checkpoint from model zoo and put it in `checkpoints/`\n# url: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n#wget http://www.linux.org.ru -P /home/alex/download\ncheckpoint_file = 'faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\ndevice = 'cuda:0'\n# init a detector\nmodel = init_detector(config_file, checkpoint_file, device=device)\n# inference the demo image\nimg = '/kaggle/input/testphotos/000.jpg'#'demo/demo.jpg'\nresult = inference_detector(model,img )\nmodel.show_result(img, result)\nprint('----- Done! -----')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## YOLACT"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/yolact/yolact_r101_1x8_coco_20200908-4cbe9101.pth #-P /home/alex/download","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#You Only Look At CoefficienTs YOLACT\n\nimport mmcv\nimport PIL.ExifTags\n# Specify the path to model config and checkpoint file\nconfig_file = '/kaggle/working/mmdetection/configs/yolact/yolact_r101_1x8_coco.py'#'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = 'yolact_r101_1x8_coco_20200908-4cbe9101.pth'\n\n# build the model from a config file and a checkpoint file\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n# test a single image and show the results\nimg = '/kaggle/input/testphotos/000.jpg'  # or img = mmcv.imread(img), which will only load it once\nresult = inference_detector(model, img)\n# visualize the results in a new window\nmodel.show_result(img, result)\n# or save the visualization results to image files\n#model.show_result(img, result, out_file='result1.jpg')\n\n# # test a video and show the results\n# video = mmcv.VideoReader('video.mp4')\n# for frame in video:\n#     result = inference_detector(model, frame)\n#     model.show_result(frame, result, wait_time=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbox_result, segm_result = result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if isinstance(result, tuple):\n    bbox_result, segm_result = result\n    if isinstance(segm_result, tuple):\n        segm_result = segm_result[0]  # ms rcnn\nelse:\n    bbox_result, segm_result = result, None\nbboxes = np.vstack(bbox_result)\nlabels = [\n        np.full(bbox.shape[0], i, dtype=np.int32)\n        for i, bbox in enumerate(bbox_result)\n]\nlabels = np.concatenate(labels)\n# draw segmentation masks\nsegms = None\nif segm_result is not None and len(labels) > 0:  # non empty\n    segms = mmcv.concat_list(segm_result)\n    if isinstance(segms[0], torch.Tensor):\n        segms = torch.stack(segms, dim=0).detach().cpu().numpy()\n    else:\n        segms = np.stack(segms, axis=0)\n\n# draw bounding boxes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segms.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bboxes.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_thr = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = bboxes[:, -1]\nbbboxes = bboxes[:, 0:4]\n\ns_mask = (scores > score_thr)\nl_mask = (labels == 0)\nresult_mask = s_mask & l_mask\n\nbbboxes = bbboxes[result_mask, :]\nlabels = labels[result_mask]\nsegms = segms[result_mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segms.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bbboxes.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = model.CLASSES\nclass_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image, ImageDraw\n\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nfrom PIL import Image\n\nTINT_COLOR = (0, 0, 0)  # Black\nTRANSPARENCY = .25  # Degree of transparency, 0-100%\nOPACITY = int(255 * TRANSPARENCY)\n\nwith Image.open(img) as im:\n    im = im.convert(\"RGBA\")\n    draw = ImageDraw.Draw(im)\n    #draw.line(result[0][0][0][0], result[0][0][0][1], fill=128)\n    #draw.line((0, im.size[1], im.size[0], 0), fill=128)\n\n    #draw.rectangle((result[0][0][0][0], result[0][0][0][1], result[0][0][0][2], result[0][0][0][3]), fill=TINT_COLOR+(OPACITY,), outline=(255, 255, 0))\n    draw.rectangle(bbboxes, fill=TINT_COLOR+(OPACITY,), outline=(255, 255, 0))\n    %matplotlib inline\n \n    imshow(np.asarray(im))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = init_detector(config_file, checkpoint_file, device='cuda:0')\ndef mask_get(img_path, model, THRESHOLD=0.5):\n    \n    draw = cv2.imread(img_path)\n    height, width, channels = draw.shape\n    #draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n    \n    result = inference_detector(model, img_path)\n    \n    if isinstance(result, tuple):\n        bbox_result, segm_result = result\n        if isinstance(segm_result, tuple):\n            segm_result = segm_result[0]  # ms rcnn\n    else:\n        bbox_result, segm_result = result, None\n    bboxes = np.vstack(bbox_result)\n    \n    labels = [\n            np.full(bbox.shape[0], i, dtype=np.int32)\n            for i, bbox in enumerate(bbox_result)\n    ]\n    labels = np.concatenate(labels)\n    \n\n    segms = None\n    if segm_result is not None and len(labels) > 0:  # non empty\n        segms = mmcv.concat_list(segm_result)\n        if isinstance(segms[0], torch.Tensor):\n            segms = torch.stack(segms, dim=0).detach().cpu().numpy()\n        else:\n            segms = np.stack(segms, axis=0)\n \n\n    scores = bboxes[:, -1]\n    b_boxes = bboxes[:, 0:4]\n\n    s_mask = (scores > THRESHOLD)\n    l_mask = (labels == 0) #только люди\n    result_mask = s_mask & l_mask\n\n    b_boxes = b_boxes[result_mask, :]\n    labels = labels[result_mask]\n    segms = segms[result_mask]\n    scores = scores[result_mask]\n    \n    mask_out = np.zeros((height, width), np.uint8)\n    for mask in  segms:\n        mask_out = mask_out | mask\n        \n    \n    return mask_out #segms, labels, b_boxes, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Готовим список маск для from mmdet.core import encode_mask_results\ndef masks_get(img_path, model, THRESHOLD=0.5):\n    \n    draw = cv2.imread(img_path)\n    height, width, channels = draw.shape\n    #draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n    \n    result = inference_detector(model, img_path)\n    \n    if isinstance(result, tuple):\n        bbox_result, segm_result = result\n        if isinstance(segm_result, tuple):\n            segm_result = segm_result[0]  # ms rcnn\n    else:\n        bbox_result, segm_result = result, None\n    bboxes = np.vstack(bbox_result)\n    \n    labels = [\n            np.full(bbox.shape[0], i, dtype=np.int32)\n            for i, bbox in enumerate(bbox_result)\n    ]\n    labels = np.concatenate(labels)\n    \n\n    segms = None\n    if segm_result is not None and len(labels) > 0:  # non empty\n        segms = mmcv.concat_list(segm_result)\n        if isinstance(segms[0], torch.Tensor):\n            segms = torch.stack(segms, dim=0).detach().cpu().numpy()\n        else:\n            segms = np.stack(segms, axis=0)\n \n\n    scores = bboxes[:, -1]\n    b_boxes = bboxes[:, 0:4]\n\n    s_mask = (scores > THRESHOLD)\n    l_mask = (labels == 0) #только люди\n    result_mask = s_mask & l_mask\n\n    b_boxes = b_boxes[result_mask, :]\n    labels = labels[result_mask]\n    segms = segms[result_mask]\n    scores = scores[result_mask]\n    \n#     mask_out = np.zeros((height, width), np.uint8)\n#     for mask in  segms:\n#         mask_out = mask_out | mask\n        \n    \n    result_ret = result[result_mask]\n    \n    return result_ret #segms, labels, b_boxes, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path = '/kaggle/input/testphotos/000.jpg'\nsegms1 = mask_get(img_path, model, THRESHOLD=0.5)\n\nim = cv2.imread(img_path)\nim = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\nres = cv2.bitwise_and(im,im,mask = segms1)\n\nplt.imshow(im)\nplt.imshow(segms1, alpha=0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Сразу сравним с эталоном')\nimage = '../input/coco2017/train2017/train2017/000000281563.jpg'\nsegms1 = mask_get(image, model, THRESHOLD=0.17)\nplt.figure(figsize=(15, 15))\nlabel = cv2.imread(\"../input/coco2017/stuffthingmaps_trainval2017/train2017/000000281563.png\")\nplt.subplot(1,2, 1)\nplt.imshow(segms1)\nplt.title(\"Predict\")\nplt.subplot(1,2, 2)\nplt.imshow(label[:, :, 0] < 0.5)\nplt.title(\"Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Из sample-submission читаем по каким именно картинкам нам нужно сделать предсказание\nsample_submission = pd.read_csv('../input/sf-dl-person-segmentation/sample-submission.csv')\nsample_submission.info()\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# кодирование маски в EncodedPixels\n\n# # encode mask results MMdetect\n#     if isinstance(result[0], tuple):\n#         result = [(bbox_results, encode_mask_results(mask_results))\n#                     for bbox_results, mask_results in result]\n#     results.extend(result)\n\ndef mask_to_rle(mask):\n    mask_flat = mask.flatten('F')\n    flag = 0\n    rle_list = list()\n    for i in range(mask_flat.shape[0]):\n        if flag == 0:\n            if mask_flat[i] == 1:\n                flag = 1\n                starts = i+1\n                rle_list.append(starts)\n        else:\n            if mask_flat[i] == 0:\n                flag = 0\n                ends = i\n                rle_list.append(ends-starts+1)\n    if flag == 1:\n        ends = mask_flat.shape[0]\n        rle_list.append(ends-starts+1)\n    #sanity check\n    if len(rle_list) % 2 != 0:\n        print('NG')\n    if len(rle_list) == 0:\n        rle = np.nan\n    else:\n        rle = ' '.join(map(str,rle_list))\n    return rle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Осталось дело за малым, \n# пройтись по списку картинок и сделать предикты с дальнейшим их кодированием в EncodedPixels\n# Это займет значительное время (около 8 часов)...\n\nTHRESHOLD=0.17  # с уровнем от которого считаеться маска - можно поиграться ПОПРОБОВАТЬ 0.3\n\nsubmit_rle_arr = []\n\nfor img_id in tqdm_notebook(sample_submission.ImageId.values):\n    image = f'../input/coco2017/val2017/{img_id}'\n    mask_out = mask_get(image, model, THRESHOLD=THRESHOLD)\n    rle = mask_to_rle(mask_out)\n    submit_rle_arr.append(rle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['EncodedPixels'] = submit_rle_arr\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------"},{"metadata":{},"cell_type":"markdown","source":"# Ниже BaseLine"},{"metadata":{},"cell_type":"markdown","source":"# Person Segmentation\n![Person Segmentation](https://cdn-images-1.medium.com/max/1200/1*UicBY4HeqWEl4l70fTtc6w.png)\n\nВ этом решении я предлагаю не изобретать велосипед...\n\nНемного погуглив можно найти готовые решения на Базе [Mask-RCNN](https://github.com/fizyr/keras-maskrcnn) или [DeepLab v3+](https://github.com/bonlime/keras-deeplab-v3-plus) с уже **предобучеными моделями!**\nКоторые отлично справляються с задачей Сегментации людией (и не только) прямо \"из коробки\". Нам остаеться только адаптировать решение именно под нашу задачу.\n\nЧто будем делать:\n> ### Берем сеть Mask-RCNN уже предобученую на датасете COCO и предсказываем маску только для класса Person \n\nЭто позволит сэкономить нам на обучении сети и сразу получить хороший результат. Звучит не плохо...\n\n![smart](http://memesmix.net/media/created/7ncr3w.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Для начала вглянем на сами картинки в датасете COCO"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Пример картинки')\nplt.figure(figsize=(15,10))\nimg = cv2.imread(\"../input/coco2017/train2017/train2017/000000281563.jpg\")\nlabel = cv2.imread(\"../input/coco2017/stuffthingmaps_trainval2017/train2017/000000281563.png\")\n# changing to the BGR format of OpenCV to RGB format for matplotlib\nplt.subplot(1,3, 1)\nplt.imshow(img[:,:,::-1])\nplt.title(\"Image\")\nplt.subplot(1,3, 2)\nplt.imshow(label)\nplt.title(\"Label\")\ndst = cv2.addWeighted(img,0.3,label,0.8,0)\nplt.subplot(1,3, 3)\nplt.imshow(dst[:,:,::-1])\nplt.title(\"Blending\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Теперь Поставим keras_maskrcnn"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras_maskrcnn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras_retinanet==0.5.1#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Скачаем веса предобученой модели\n!wget https://github.com/fizyr/keras-maskrcnn/releases/download/0.2.2/resnet50_coco_v0.2.0.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Копипастим пример\nу keras_maskrcnn есть отличный [Пример](https://github.com/fizyr/keras-maskrcnn/blob/master/examples/ResNet50MaskRCNN.ipynb)  \nПросто копируем код из примера, чтоб понять как все работает и посмотреть результаты с нашей картинкой..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# show images inline\n%matplotlib inline\n\n# automatically reload modules when they have changed\n%load_ext autoreload\n%autoreload 2\n\n# # import keras\nimport keras\n\n# import keras_retinanet\nfrom keras_maskrcnn import models\nfrom keras_maskrcnn.utils.visualization import draw_mask\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption, draw_annotations\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.colors import label_color\n\n# import miscellaneous modules\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport numpy as np\nimport time\n\n# set tf backend to allow memory to grow, instead of claiming everything\n# import tensorflow as tf\n\n# def get_session():\n#     config = tf.ConfigProto()\n#     config.gpu_options.allow_growth = True\n#     return tf.Session(config=config)\n\n# use this environment flag to change which GPU to use\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\n# set the modified tf session as backend in keras\n#keras.backend.tensorflow_backend.set_session(get_session())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adjust this to point to your downloaded/trained model\nmodel_path = os.path.join('./','resnet50_coco_v0.2.0.h5')\n\n# load retinanet model\nmodel = models.load_model(model_path, backbone_name='resnet50')\nprint(model.summary())\n\n# load label to names mapping for visualization purposes\nlabels_to_names = {0: 'person',}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load image\nimage = read_image_bgr('/kaggle/input/testphotos/000.jpg')#('../input/coco2017/train2017/train2017/000000281563.jpg')\n\n# copy to draw on\ndraw = image.copy()\ndraw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n# preprocess image for network\nimage = preprocess_image(image)\nimage, scale = resize_image(image)\n\n# process image\nstart = time.time()\noutputs = model.predict_on_batch(np.expand_dims(image, axis=0))\nprint(\"processing time: \", time.time() - start)\n\nboxes  = outputs[-4][0]\nscores = outputs[-3][0]\nlabels = outputs[-2][0]\nmasks  = outputs[-1][0]\n\n\n# correct for image scale\nboxes /= scale\n\n# visualize detections\nfor box, score, label, mask in zip(boxes, scores, labels, masks):\n    if score < 0.5:\n        break\n\n    color = label_color(label)\n    \n    b = box.astype(int)\n    draw_box(draw, b, color=color)\n    \n    mask = mask[:, :, label]\n    draw_mask(draw, b, mask, color=label_color(label))\n    \n    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n    #draw_caption(draw, b, caption)\n    \nplt.figure(figsize=(15, 15))\nplt.axis('off')\nplt.imshow(draw)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for box, score, label, mask in zip(boxes, scores, labels, masks):\nprint(boxes.shape)\nprint(scores.shape)\nprint(labels.shape)\nprint(masks.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Смотриться не плохо, давай теперь вынесем только маску и сравним с эталоном.\nдля этого чуть модифицируем код из примера (уберем боксы и оставим только маску) и завернем все в функцию чтоб было удобно с этим работать в дальнейшем"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask_get (image, model, THRESHOLD=0.5):\n    \n    #image = read_image_bgr(image)\n    # copy to draw on\n    draw = image.copy()\n    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n    # preprocess image for network\n    image = preprocess_image(image)\n    image, scale = resize_image(image)\n\n    # process image\n    start = time.time()\n    outputs = model.predict_on_batch(np.expand_dims(image, axis=0))\n    #print(\"processing time: \", time.time() - start)\n    draw = np.zeros((draw.shape[0], draw.shape[1], 3), np.uint8)\n\n    boxes  = outputs[-4][0]\n    scores = outputs[-3][0]\n    labels = outputs[-2][0]\n    masks  = outputs[-1][0]\n\n    # correct for image scale\n    boxes /= scale\n\n    # visualize detections\n    #draw = np.zeros((image.shape[0], image.shape[1], 3), np.uint8)\n    for box, score, label, mask in zip(boxes, scores, labels, masks):\n        if score < THRESHOLD:\n            break\n        b = box.astype(int)\n        #draw_box(draw, b, color=color)\n        if label == 0:\n            mask = mask[:, :, label]\n            #draw = np.zeros((image.shape[0], image.shape[1], 3), np.uint8)\n            draw_mask(draw, b, mask, color=label_color(label))\n\n        #caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n        #draw_caption(draw, b, caption)\n\n    mask_out = (draw[:, :, 0] > THRESHOLD).astype(np.uint8)\n    return(mask_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Сразу сравним с эталоном')\nimage = read_image_bgr('../input/coco2017/train2017/train2017/000000281563.jpg')\nmask_out = mask_get(image, model, THRESHOLD=0.5)\nplt.figure(figsize=(15, 15))\nlabel = cv2.imread(\"../input/coco2017/stuffthingmaps_trainval2017/train2017/000000281563.png\")\nplt.subplot(1,2, 1)\nplt.imshow(mask_out)\nplt.title(\"Predict\")\nplt.subplot(1,2, 2)\nplt.imshow(label[:, :, 0] < 0.5)\nplt.title(\"Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Даже на таком сложном примере получаем неплохой Результат! \n> Кстати, ты заметил что разметка самого датасета COCO далека от идеала, и в некоторых местах модель это делает даже лучше!  \n> Это известная плоблема этого датасета, по этому для сегментации людей используют специализированные датасеты (Например [Supervisely Person Dataset](https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469)), но для обучения нам и COCO сойдет\n\nНам осталось только сделать предикт на каждую картинку и записать в submission в нужном формате."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Из sample-submission читаем по каким именно картинкам нам нужно сделать предсказание\nsample_submission = pd.read_csv('../input/sf-dl-person-segmentation/sample-submission.csv')\nsample_submission.info()\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# кодирование маски в EncodedPixels\ndef mask_to_rle(mask):\n    mask_flat = mask.flatten('F')\n    flag = 0\n    rle_list = list()\n    for i in range(mask_flat.shape[0]):\n        if flag == 0:\n            if mask_flat[i] == 1:\n                flag = 1\n                starts = i+1\n                rle_list.append(starts)\n        else:\n            if mask_flat[i] == 0:\n                flag = 0\n                ends = i\n                rle_list.append(ends-starts+1)\n    if flag == 1:\n        ends = mask_flat.shape[0]\n        rle_list.append(ends-starts+1)\n    #sanity check\n    if len(rle_list) % 2 != 0:\n        print('NG')\n    if len(rle_list) == 0:\n        rle = np.nan\n    else:\n        rle = ' '.join(map(str,rle_list))\n    return rle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Осталось дело за малым, \n# пройтись по списку картинок и сделать предикты с дальнейшим их кодированием в EncodedPixels\n# Это займет значительное время (около 8 часов)...\n\nTHRESHOLD=0.5  # с уровнем от которого считаеться маска - можно поиграться\n\nsubmit_rle_arr = []\n\nfor img_id in tqdm_notebook(sample_submission.ImageId.values):\n    image = read_image_bgr(f'../input/coco2017/val2017/{img_id}')\n    mask_out = mask_get(image, model, THRESHOLD=THRESHOLD)\n    rle = mask_to_rle(mask_out)\n    submit_rle_arr.append(rle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['EncodedPixels'] = submit_rle_arr\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Вот и все решение!**\n\n### Как можно улучшить результат:\n1. Подобрать THRESHOLD\n2. Дообучить модель для более точной разметки только класса Person\n3. Сделать тоже самое например на [DeepLab v3+](https://github.com/bonlime/keras-deeplab-v3-plus) и усреднить результаты (те сделать ансамбль моделей, этож kaggle, куда тут без этого ;) )"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}